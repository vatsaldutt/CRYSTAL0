{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1360 768\n"
     ]
    }
   ],
   "source": [
    "from screeninfo import get_monitors\n",
    "width = get_monitors()[0].width\n",
    "height = get_monitors()[0].height\n",
    "print(width, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def web_scrapper(input_data_to_search):\n",
    "    source = requests.get('https://www.google.com/search?q=' + input_data_to_search).text\n",
    "    soup = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "    ans = None\n",
    "    list1 = []\n",
    "    list1 = []\n",
    "\n",
    "    for i in soup.find_all('div', class_='BNeawe s3v9rd AP7Wnd'):\n",
    "        for j in i.find_all('div', class_='BNeawe s3v9rd AP7Wnd'):\n",
    "            list1.append(j.text)\n",
    "            list1.append(str(j.text).split(' '))\n",
    "            try:\n",
    "                ans = soup.find('div', class_='BNeawe iBp4i AP7Wnd').text\n",
    "            except AttributeError:\n",
    "                if list1[-1][-1] == 'Wikipedia':\n",
    "                    ans = j.text\n",
    "\n",
    "                else:\n",
    "                    try:\n",
    "                        if \"\\n\" in list1[1]:\n",
    "                            ans = list1[0].split('\\n')[1]\n",
    "                        else:\n",
    "                            if list1[0] == 'noun\\n':\n",
    "                                ans = list1[1]\n",
    "                            elif list1[0] == 'adjective\\n':\n",
    "                                ans = list1[1]\n",
    "                            elif list1[0] == 'verb\\n':\n",
    "                                ans = list1[1]\n",
    "                            elif list1[0] == 'adverb\\n':\n",
    "                                ans = list1[1]\n",
    "                            elif list1[0] == 'preposition\\n':\n",
    "                                ans = list1[1]\n",
    "                            else:\n",
    "                                for text in range(len(list1)):\n",
    "                                    if ':' not in list1[text] and '·' not in list1[text] and '...' not in list1[text]:\n",
    "                                        ans = list1[text]\n",
    "                                    elif ':' not in list1[text] and '·' not in list1[text]:\n",
    "                                        ans = list1[text]\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "    if ans is None:\n",
    "        ans = list1[0]\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LAMP',\n",
       " 'is',\n",
       " 'an',\n",
       " 'open-source',\n",
       " 'Web',\n",
       " 'development',\n",
       " 'platform',\n",
       " 'that',\n",
       " 'uses',\n",
       " 'Linux',\n",
       " 'as',\n",
       " 'the',\n",
       " 'operating',\n",
       " 'system,',\n",
       " 'Apache',\n",
       " 'as',\n",
       " 'the',\n",
       " 'Web',\n",
       " 'server,',\n",
       " 'MySQL',\n",
       " 'as',\n",
       " 'the',\n",
       " 'relational',\n",
       " 'database\\xa0...']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_scrapper(\"what is lamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "\n",
    "def web_scrapper(input_data_to_search):\n",
    "    source = requests.get('https://www.google.com/search?q=' + input_data_to_search).text\n",
    "    soup = BeautifulSoup(source, 'lxml')\n",
    "    part_of_speeches = ['noun', 'adjective', 'verb', 'adverb', 'pronoun', 'preposition', 'conjunction', 'interjection']\n",
    "\n",
    "    list1 = []\n",
    "\n",
    "    for i in soup.find_all('div', class_='BNeawe s3v9rd AP7Wnd'):\n",
    "        for j in i.find_all('div', class_='BNeawe s3v9rd AP7Wnd'):\n",
    "            list1.append(j.text)\n",
    "    \n",
    "    try:\n",
    "        return soup.find('div', class_='BNeawe iBp4i AP7Wnd').text\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if list1[0].split()[0] in part_of_speeches:\n",
    "        return 'As a '+list1[0].split()[0]+' it means '+list1[1]\n",
    "    \n",
    "    for text in list1:\n",
    "        if text.split()[-1] == 'Wikipedia':\n",
    "            return 'According to the Wikipedia '+str('/'.join(text.split()[0:-1]).replace('/', ' '))\n",
    "    answer_types = ['You would say that ', 'That would be ', \"That's \"]\n",
    "    for i in soup.find_all('div'):\n",
    "        for j in i.find_all('div'):\n",
    "            for k in j.find_all('div'):\n",
    "                for m in k.find_all('div'):\n",
    "                    if 'MUxGbd u31kKd gsrt lyLwlc' in str(m):\n",
    "                        translation = str(m.text).replace('Translation', '').replace('Translate', '')\n",
    "    try:\n",
    "        return random.choice(answer_types) + translation\n",
    "    \n",
    "    except:\n",
    "        urls = []\n",
    "        for a in soup.find_all('a'):\n",
    "            if a.get('href')[0:15] == '/url?q=https://':\n",
    "                url = a.get('href').replace('/url?q=https://', '')\n",
    "                urls.append(url[0: url.index('&sa')])\n",
    "        \n",
    "        for u in range(len(urls)):\n",
    "            urls[u] = 'https://'+urls[u]\n",
    "        return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.wyzant.com/resources/answers/561242/what-is-2x-2x-4',\n",
       " 'https://www.mathway.com/popular-problems/Algebra/206803',\n",
       " 'https://www.mathway.com/popular-problems/Algebra/220533',\n",
       " 'https://www.tiger-algebra.com/drill/2x4-2x/',\n",
       " 'https://www.tiger-algebra.com/drill/2x-4%3D0/',\n",
       " 'https://socratic.org/questions/how-do-you-solve-2x-2-2x-4',\n",
       " 'https://socratic.org/questions/how-do-you-solve-2x-2x-4-3x-x-2',\n",
       " 'https://mathsolver.microsoft.com/en/topic/algebra/expand/solve/7x(2x-4)',\n",
       " 'https://www.symbolab.com/solver/step-by-step/expand%2520(2x-4)(x-5)',\n",
       " 'https://www.geteasysolution.com/24-2x%3D4',\n",
       " 'https://support.google.com/websearch%3Fp%3Dws_settings_location%26hl%3Den',\n",
       " 'https://accounts.google.com/ServiceLogin%3Fcontinue%3Dhttps://www.google.com/search%253Fq%253D2x%252B2x%25253D4%26hl%3Den']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_scrapper('2x+2x=4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import sys \n",
    "\n",
    "soup = BeautifulSoup(requests.get('https://www.google.com/search?q=' + \"2+2\").text, 'html')\n",
    "for foo in soup.find_all('div', attrs={'class': 'easter-egg'}):\n",
    "    print(foo.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width/4.923076923076923, height/7.714285714285714, width/5.80060422960725, height/18.0, \n"
     ]
    }
   ],
   "source": [
    "data = '390, 140, 331, 60'\n",
    "c = 0\n",
    "final = []\n",
    "final2 = ''\n",
    "for i in data.split():\n",
    "    numbers = i.replace(',', '').replace(')', '').replace('(', '')\n",
    "    if str(c/2)[-1] == '0':\n",
    "        final.append('width/'+str(1920/int(numbers)))\n",
    "    else:\n",
    "        final.append('height/'+str(1080/int(numbers)))\n",
    "    c += 1\n",
    "for j in final:\n",
    "    final2 = final2 + (j.replace(\"'\", \"\")) + \", \"\n",
    "print(final2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "data = pandas.read_pickle('face_rec')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "source\n",
    "for a in soup.find_all('a'):\n",
    "    if 'translate' in str(a).lower():\n",
    "        print(a)\n",
    "    if a.get('href')[0:15] == '/url?q=https://':\n",
    "        url = a.get('href').replace('/url?q=https://', '')\n",
    "        urls.append(url[0: url.index('&sa')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Over 60% of Indians understand and speak Hindi\n",
      " South Indians are the LEAST likely to speak and understand Hindi\n",
      " Most people in India, especially those in the tourism industry, can understand and speak English very well\n",
      " Instead, a good way to start is to memorize certain key phrases that you can practice during your trip\n",
      " It's best if you choose a few to practice during your travels\n",
      " Namaste can be used at any time of the day\n",
      " You may also hear some people say namaskar (nam-as-scar) which can be used interchangeably with namaste\n",
      " Today many young people in India will just use the English salutations and save namaste for elders or highly respected individuals\n",
      " If you master this, the locals will think you are very polite\n",
      " This sound is pronounced by starting to say the 'buh' sound and then pushing more air out to make a 'huh' sound immediately afterward\n",
      " Many people will also use namaste for this greeting\n",
      " This is a great way to greet friends, close family members, or people similar in age to you\n",
      " This word is borrowed from Urdu, Hindi's sister language, and is formal and serious-sounding\n",
      " In all stores where prices are not written on items, it is perfectly acceptable to barter and the seller will likely quote a high price initially to see if you will pay it\n",
      " But be prepared! If you ask, you are entering into a bartering session so it's a good idea to only ask the price on items you are honestly thinking of buying\n",
      " You can add kripaya to the beginning of your English sentences when asking for directions or information to sound respectful\n",
      " This phrase is taken directly from the ancient Sanskrit language and is not often used\n",
      " However, in Hindi, there is no direct translation for this English phrase\n",
      " This phrase can also be used if you need to interrupt someone or get their attention to ask a question\n",
      " Because of this, Hindi includes a lot of honorifics or titles that are used to show respect to someone based on their societal position or age\n",
      " Instead, there are a few different honorifics that can be used in different situations\n",
      " Ji can be used for people of both genders and is placed at the end of someone's name to indicate your respect for that person\n",
      " If you want to refer to someone who is older than you, you can use the terms 'auntie' or 'uncle'\n",
      " Indians will often refer to women middle-aged and older as aunties and men of the same age as uncles\n",
      " For example, in the state of Assam, most people speak Assamese and in the state of Bengal, most people speak Bengali\n",
      " Therefore, many South Indians do not speak Hindi or learn it in school\n",
      " Most travelers have few issues when communicating with English in  South India\n",
      " You can have any trip tailor made for your travel\n",
      "\n",
      "\n",
      " Not just any journey, but the unique trip with the exceptional experiences you're looking for â whether it's a family vacation, a honeymoon, or your annual break\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mainURL = urls[0]\n",
    "urlsource = requests.get('https://'+str(mainURL)).text\n",
    "urlsoup = BeautifulSoup(urlsource, 'html.parser')\n",
    "for sentence in urlsoup.find('body').text.split('.'):\n",
    "    if '\\n' not in sentence:\n",
    "        print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_62656/329645633.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconjunctions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mphraseList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mphraseList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphraseList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "phrase = 'My name is Vatsal Dutt. I am from India. I like coding, but coding is addicting and I also like chess and chess is also addicting'\n",
    "conjunctions = ['and', 'but', 'for', 'nor', 'or', 'so', 'yet',',', '.']\n",
    "phraseList = []\n",
    "phraseList.append(phrase)\n",
    "\n",
    "for word in conjunctions:\n",
    "    for i in phraseList:\n",
    "        try:\n",
    "            phraseList[phraseList.index(i)] = i.split(word)\n",
    "        except AttributeError:\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Session cannot generate requests",
     "output_type": "error",
     "traceback": [
      "Error: Session cannot generate requests",
      "at w.executeCodeCell (/home/crystal/.vscode/extensions/ms-toolsai.jupyter-2021.9.1001312534/out/client/extension.js:52:301180)",
      "at w.execute (/home/crystal/.vscode/extensions/ms-toolsai.jupyter-2021.9.1001312534/out/client/extension.js:52:300551)",
      "at w.start (/home/crystal/.vscode/extensions/ms-toolsai.jupyter-2021.9.1001312534/out/client/extension.js:52:296215)",
      "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
      "at async t.CellExecutionQueue.executeQueuedCells (/home/crystal/.vscode/extensions/ms-toolsai.jupyter-2021.9.1001312534/out/client/extension.js:52:310950)",
      "at async t.CellExecutionQueue.start (/home/crystal/.vscode/extensions/ms-toolsai.jupyter-2021.9.1001312534/out/client/extension.js:52:310490)"
     ]
    }
   ],
   "source": [
    "phraseList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from functools import lru_cache\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium import webdriver\n",
    "import random\n",
    "\n",
    "@lru_cache(maxsize=10)\n",
    "def web_scrapper(input_data_to_search):\n",
    "    page_url = 'https://www.google.com/search?q=' + input_data_to_search\n",
    "    source = requests.get(page_url).text\n",
    "    soup = BeautifulSoup(source, 'html.parser')\n",
    "    part_of_speeches = ['noun', 'adjective', 'verb', 'adverb', 'pronoun', 'preposition', 'conjunction', 'interjection', 'exclamation', 'numeral', 'article', 'determiner']\n",
    "\n",
    "    list1 = []\n",
    "\n",
    "    for i in soup.find_all('div', class_='BNeawe s3v9rd AP7Wnd'):\n",
    "        for j in i.find_all('div', class_='BNeawe s3v9rd AP7Wnd'):\n",
    "            list1.append(j.text)\n",
    "    \n",
    "    try:\n",
    "        return soup.find('div', class_='BNeawe iBp4i AP7Wnd').text\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if list1[0].split()[0] in part_of_speeches:\n",
    "        if list1[0].split()[0][0] == \"a\":\n",
    "            return 'As an '+list1[0].split()[0]+' it means '+list1[1]\n",
    "        \n",
    "        else:\n",
    "            return 'As a '+list1[0].split()[0]+' it means '+list1[1]\n",
    "    \n",
    "    answer_types = ['You would say that ', 'That would be ', \"That's \"]\n",
    "    for i in soup.find_all('div'):\n",
    "        for j in i.find_all('div'):\n",
    "            for k in j.find_all('div'):\n",
    "                for m in k.find_all('div'):\n",
    "                    if 'MUxGbd u31kKd gsrt lyLwlc' in str(m):\n",
    "                        translation = str(m.text).replace('Translation', '').replace('Translate', '')\n",
    "    try:\n",
    "        return random.choice(answer_types) + translation\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        PATH = \"D:/chromedriver.exe\"\n",
    "        op = webdriver.ChromeOptions()\n",
    "        op.add_argument('headless')\n",
    "        driver = webdriver.Chrome(options=op, service=Service(PATH))\n",
    "        driver.get(page_url)\n",
    "        algebra_result = driver.find_elements_by_class_name('LPBLxf')\n",
    "        return algebra_result[-1].text.split('\\n')[-1]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if \"Duration\" not in list1[0]:\n",
    "        if len(list1[0].split()) > 10:\n",
    "            try:\n",
    "                return list1[0].split('...')[0].split(\"·\")[1]\n",
    "            \n",
    "            except:\n",
    "                return list1[0].split('...')[0]\n",
    "            \n",
    "\n",
    "    for text in list1:\n",
    "        list_text = text.split()\n",
    "        if len(list_text) != 0:\n",
    "            if list_text[-1] == 'Wikipedia':\n",
    "                return 'According to the Wikipedia, '+str('/'.join(text.split()[0:-1]).replace('/', ' '))\n",
    "    urls = []\n",
    "    for a in soup.find_all('a'):\n",
    "        if a.get('href')[0:15] == '/url?q=https://':\n",
    "            url = a.get('href').replace('/url?q=https://', '')\n",
    "            urls.append(url[0: url.index('&sa')])\n",
    "    \n",
    "    for u in range(len(urls)):\n",
    "        urls[u] = 'https://'+urls[u]\n",
    "\n",
    "    url_source = requests.get(urls[0]).text\n",
    "    soup = BeautifulSoup(url_source, 'html.parser')\n",
    "    url_text = []\n",
    "    for i in soup.find_all(\"p\"):\n",
    "        url_text.append(i.text)\n",
    "\n",
    "    paracount = 0\n",
    "    for j in url_text:\n",
    "        if len(j.split()) < 11:\n",
    "            pass\n",
    "        \n",
    "        elif paracount == 1:\n",
    "            return \"According to the website \"+urls[0].split('/')[2]+\", \"+j.split(\"\\r\\n\\r\")[0]\n",
    "\n",
    "        else:\n",
    "            paracount += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vatdu\\AppData\\Local\\Temp/ipykernel_11028/1495789865.py:52: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n",
      "  algebra_result = driver.find_elements_by_class_name('LPBLxf')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"gTTS (Google Text-to-Speech), a Python library and CLI tool to interface with Google Translate's text-to-speech API. Writes spoken mp3 data to a file,\\xa0\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_scrapper(\"gtts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-c42d80094b34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://www.google.com/search?q='\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"how do stars form\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mtext_cookie\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_elements_by_class_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'LPBLxf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_cookie\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium import webdriver\n",
    "\n",
    "PATH = \"D:/chromedriver.exe\"\n",
    "op = webdriver.ChromeOptions()\n",
    "op.add_argument('headless')\n",
    "driver = webdriver.Chrome(options=op, service=Service(PATH))\n",
    "\n",
    "driver.get('https://www.google.com/search?q='+\"how do stars form\")\n",
    "text_cookie = driver.find_elements_by_class_name('LPBLxf')\n",
    "print(text_cookie[-1].text.split('\\n')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simplify 2x×2x\n",
      "2x + 2x = 4x\n",
      "2x 2x answer\n",
      "2x 2x\n",
      "2x 2x equals\n",
      "2x multiplied by 2x\n",
      "2x 2x 1\n",
      "−2x+2x\n"
     ]
    }
   ],
   "source": [
    "source = requests.get('https://www.google.com/search?q=2x+%2D+2x').text\n",
    "soup = BeautifulSoup(source, 'html.parser')\n",
    "\n",
    "for a in soup.findAll('div', class_='BNeawe s3v9rd AP7Wnd lRVwie'):\n",
    "    print(a.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is an interpreted, object-oriented, high-level programming\n",
      "language with dynamic semantics.  Its high-level built in data\n",
      "structures, combined with dynamic typing and dynamic binding, make it\n",
      "very attractive for Rapid Application Development, as well as for use\n",
      "as a scripting or glue language to connect existing components\n",
      "together.  Python's simple, easy to learn syntax emphasizes\n",
      "readability and therefore reduces the cost of program maintenance.\n",
      "Python supports modules and packages, which encourages program\n",
      "modularity and code reuse.  The Python interpreter and the extensive\n",
      "standard library are available in source or binary form without charge\n",
      "for all major platforms, and can be freely distributed.\n"
     ]
    }
   ],
   "source": [
    "source = requests.get(\"https://www.python.org/doc/essays/blurb/\").text\n",
    "soup = BeautifulSoup(source, 'html.parser')\n",
    "url_text = []\n",
    "for i in soup.find_all(\"p\"):\n",
    "    url_text.append(i.text)\n",
    "\n",
    "paracount = 0\n",
    "\n",
    "for j in url_text:\n",
    "    if len(j.split()) < 11:\n",
    "        pass\n",
    "    \n",
    "    elif paracount == 1:\n",
    "        print(j.split(\"\\r\\n\\r\")[0])\n",
    "        break\n",
    "\n",
    "    else:\n",
    "        paracount += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
